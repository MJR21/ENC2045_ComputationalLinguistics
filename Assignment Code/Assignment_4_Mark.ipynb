{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "answering-warren",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.43s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.00s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.55it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.92it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.90it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.22s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.26s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.96it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.39s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 200.57it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.51s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.90it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.29s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.55it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.48s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.28it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.36s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.75it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.97it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.84it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.59it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.66it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.27it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.05it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.66it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.51it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.33it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.29it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.57it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEMMAS</th>\n",
       "      <th>FREQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>真的</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>沒有</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>覺得</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>知道</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看到</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>現在</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>喜歡</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>朋友</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>一直</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>其實</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>不會</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>發現</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>男友</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>一下</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>已經</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>分享</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>工作</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>很多</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>時間</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>感覺</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>第一</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LEMMAS  FREQ\n",
       "0      真的   116\n",
       "1      沒有    93\n",
       "2      覺得    90\n",
       "3      知道    70\n",
       "4      看到    67\n",
       "5      現在    63\n",
       "6      喜歡    56\n",
       "7      朋友    55\n",
       "8      一直    52\n",
       "9      其實    52\n",
       "10     不會    50\n",
       "11     發現    43\n",
       "12     男友    42\n",
       "13     一下    42\n",
       "14     已經    41\n",
       "15     分享    41\n",
       "16     工作    40\n",
       "17     很多    40\n",
       "18     時間    40\n",
       "19     感覺    39\n",
       "20     第一    38"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "#this makes your code more universal. Allows us to arrive at the working directory of whoever wants to run this script t\n",
    "path = os.getcwd() \n",
    "\n",
    "# the line below assumes whoever is running this script has the data in question loaded in a demo_data folder in their working directory\n",
    "data = pd.read_csv(path + '/demo_data/dcard-top100.csv')\n",
    "\n",
    "# similarly, there should be a list of stop words in the demo_data folder that this code can read in\n",
    "read_stoplist = open(path +'/demo_data/stopwords/tomlinNTUB-chinese-stopwords.txt', encoding = \"utf-8\")\n",
    "\n",
    "#remove \\n lines from stops\n",
    "stoplist = [line.rstrip('\\n') for line in read_stoplist]\n",
    "\n",
    "#preparing our preprocessing tools\n",
    "import ckip_transformers\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "import unicodedata\n",
    "ws_driver = CkipWordSegmenter(level=3, device=-1) #note - 'level=1' will run much faster, so initialize level 1 while still experimenting with code\n",
    "\n",
    "#here, we have a function that will take the data, run in through a pipeline, and yield our desired results. \n",
    "def text_freq_counter(text):  \n",
    "    lemmas = []\n",
    "    for i in text:\n",
    "        #The line below removes all non-chinese characters\n",
    "        non_chin_rem = [''.join([c for c in i if unicodedata.category(c)[:2] in [\"Lo\"]])]\n",
    "        #Because of the above line of code, some of the content was completely erased leaving empty lists\n",
    "        #The following for-loop skips over any empty iterables (iterables = rows in the column of data)\n",
    "        for i in non_chin_rem:\n",
    "            if i == \"\":\n",
    "                continue\n",
    "            #now we can segment our data with the ckip transformer\n",
    "            ws = ws_driver(non_chin_rem)\n",
    "            #the segmenter created a lot of nested lists. the line below unnests them.\n",
    "            flat = [i for j in ws for i in j]\n",
    "            #remove the stop words from our list\n",
    "            stop_rem = [i for i in flat if i not in stoplist]\n",
    "            # put all segmented words into a list\n",
    "            lemmas.append(stop_rem)\n",
    "        #again, we have a list of lists, so we unnest them\n",
    "        flat_lemmas = [i for j in lemmas for i in j]\n",
    "        #filter for bigrams\n",
    "        bigrams = [i for i in flat_lemmas if len(i) == 2]\n",
    "        #create a data frame\n",
    "        df_lem = pd.DataFrame(bigrams, columns = ['LEMMAS'])        \n",
    "        #get frequency counts\n",
    "        freq = df_lem[\"LEMMAS\"].value_counts().reset_index()\n",
    "        #name columns\n",
    "        freq.columns = [\"LEMMAS\",\"FREQ\"]\n",
    "    #final object is a dataframe of bigram frequencies\n",
    "    return freq\n",
    "\n",
    "# create a variable that save the data we run through the function we created above\n",
    "Q4_1 = text_freq_counter(data.content)\n",
    "#print our results\n",
    "Q4_1.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "direct-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 336.57it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.98s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.79s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.21it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.06s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.60it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.51s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1004.38it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.87s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.37it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.12s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.60s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.73s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.96s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.94s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.38s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.09s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.41s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.49s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.55s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.77s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.52s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.51s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.60s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  3.00s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.30s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.89s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 167.09it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:26<00:00, 13.40s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.90it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.78s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.70s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.51s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.52s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.74s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.31it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.84s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.53it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.44s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.35s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.74s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.06s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.45s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.73it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.53it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.47s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.70s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.79s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.16it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.55it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.14s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.79s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.11s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.10s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.93s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "Tokenization: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.01s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.43s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.53it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.90s/it]\n",
      "Tokenization: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>台灣</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>台南</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>台</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>英國</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>台中</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>韓國</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>沖繩</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>聖圭</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>德國</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>台北</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SHINee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>資生堂</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chanel</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Emily</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Celine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>板橋</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>武林</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>東京</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>杜克大學</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>台大</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Named Entities  Freq\n",
       "0              台灣    23\n",
       "1              日本    18\n",
       "2              台南     7\n",
       "3               台     6\n",
       "4              英國     5\n",
       "5              台中     4\n",
       "6              韓國     4\n",
       "7              沖繩     4\n",
       "8              聖圭     4\n",
       "9              德國     3\n",
       "10             台北     3\n",
       "11         SHINee     2\n",
       "12            資生堂     2\n",
       "13         Chanel     2\n",
       "14          Emily     2\n",
       "15         Celine     2\n",
       "16             板橋     2\n",
       "17             武林     2\n",
       "18             東京     2\n",
       "19           杜克大學     2\n",
       "20             台大     2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#initialize the named entity recognition segmenter. Again, notice the level\n",
    "ner_driver = CkipNerChunker(level=3, device=-1)\n",
    "\n",
    "#create a function below that will yield our desired results\n",
    "def ner_freq(data):\n",
    "    ner_list = []\n",
    "    for i in data:\n",
    "        #remove symbols\n",
    "        cleaned = [''.join([c for c in i if unicodedata.category(c)[0] not in [\"S\"]])]\n",
    "        #extract named entities\n",
    "        ner = ner_driver(cleaned, use_delim = True)\n",
    "        #put into empty list prepared before the current for-loop\n",
    "        ner_list.append(ner)\n",
    "    #results from previous for-loop yielded nested lists within lists. Function belows flattens them.\n",
    "    unnest = [i for j in ner_list for i in j]\n",
    "    #use a for-loop to extract the named entities from within the sentences\n",
    "    ner_ext = []\n",
    "    for sentence_ner in unnest:\n",
    "        for entity in sentence_ner:\n",
    "            ner_ext.append(entity)\n",
    "    pre_ner_df = []\n",
    "    #filter the named entities to only include the locations and organizations \n",
    "    for i in ner_ext:\n",
    "        if i[1] == 'GPE':\n",
    "            pre_ner_df.append(i[0])\n",
    "        if i[1] == 'ORG':\n",
    "            pre_ner_df.append(i[0])\n",
    "    #create data frame and get frequencies\n",
    "    df = pd.DataFrame(pre_ner_df, columns = ['Named Entities'])\n",
    "    freq = df[\"Named Entities\"].value_counts().reset_index()\n",
    "    freq.columns = ['Named Entities', 'Freq']\n",
    "    return freq\n",
    "\n",
    "Q4_2 = ner_freq(data.content)\n",
    "Q4_2.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hairy-stocks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   N-V  FREQ\n",
      "0   知道    70\n",
      "1   看到    67\n",
      "2   喜歡    55\n",
      "3   朋友    54\n",
      "4   分享    43\n",
      "5   男友    42\n",
      "6   工作    40\n",
      "7   感覺    38\n",
      "8   沒有    38\n",
      "9   希望    35\n",
      "10  發現    34\n",
      "11  今天    33\n",
      "12  時間    33\n",
      "13  感情    33\n",
      "14  蛋糕    31\n",
      "15  出去    30\n",
      "16  想要    30\n",
      "17  公司    29\n",
      "18  問題    28\n",
      "19  故事    27\n",
      "20  關係    27\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#import necessary modules and tools for processing\n",
    "import spacy\n",
    "\n",
    "#the code below resets the default stop list in spacy to match the one provided by Alvin \n",
    "custom_stopwd = set(stoplist)\n",
    "override = spacy.util.get_lang_class('zh')\n",
    "override.Defaults.stop_words = custom_stopwd\n",
    "from spacy.lang.zh.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('zh_core_web_trf')\n",
    "\n",
    "#create function to produce freqency count of bigram nouns and verbs\n",
    "def nvfreq(data):\n",
    "    #strip out non-chinese characters\n",
    "    chin_only = [''.join([c for c in i if unicodedata.category(c)[:2] in [\"Lo\"]]) for i in data]\n",
    "    #ust spacy parser\n",
    "    n = [nlp(i) for i in chin_only]\n",
    "    toks = []\n",
    "    #extract nouns and verbs\n",
    "    for lists in n:\n",
    "        for tokens in lists:\n",
    "            if tokens.pos_ == \"NOUN\":\n",
    "                toks.append(tokens)\n",
    "            if tokens.pos_ == \"VERB\":\n",
    "                toks.append(tokens)\n",
    "    #filter extracted nouns and verbs so that they are only bigrams\n",
    "    bigram = [i for i in toks if len(i)==2]\n",
    "    #remove stops utilizing the stopwords we replaced with the default earlier\n",
    "    filt_bigram = [i for i in bigram if i.is_stop==False]\n",
    "    #extract just the strings, because the elements in the filt_bigram list are spacy tokens (differnt kind of object that cannot be processed with the counting method below. \n",
    "    count_obj = [i.text for i in filt_bigram]\n",
    "    #create a dataframe and produce frequencies\n",
    "    df = pd.DataFrame(count_obj, columns = ['N-V'])\n",
    "    freq = df[\"N-V\"].value_counts().reset_index()\n",
    "    freq.columns = ['N-V', 'FREQ']\n",
    "    return freq\n",
    "\n",
    "#Run data through function and produce results \n",
    "Q4_3 = nvfreq(data.content)\n",
    "print(Q4_3.head(21))\n",
    "\n",
    "#generate a wordcloud \n",
    "from wordcloud import WordCloud\n",
    "vapor = Q4_3.set_index('N-V').to_dict()['FREQ']\n",
    "cloud = WordCloud().generate_from_frequencies(vapor)\n",
    "image = cloud.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surrounded-jason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJ-PRED</th>\n",
       "      <th>FREQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我_喜歡</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我_想</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>他_說</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我_覺</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我_知道</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>我_看</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>我_看到</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>我_愛</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>我_用</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>大家_好</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>我_說</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>我_去</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>你_有</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>我_有</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>他_覺</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>她_說</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>我_問</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>我_決定</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>我_在</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>我_要</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>他_有</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJ-PRED  FREQ\n",
       "0       我_喜歡    21\n",
       "1        我_想    20\n",
       "2        他_說    20\n",
       "3        我_覺    19\n",
       "4       我_知道    16\n",
       "5        我_看    14\n",
       "6       我_看到    11\n",
       "7        我_愛    10\n",
       "8        我_用     9\n",
       "9       大家_好     8\n",
       "10       我_說     8\n",
       "11       我_去     7\n",
       "12       你_有     7\n",
       "13       我_有     7\n",
       "14       他_覺     6\n",
       "15       她_說     6\n",
       "16       我_問     6\n",
       "17      我_決定     6\n",
       "18       我_在     6\n",
       "19       我_要     6\n",
       "20       他_有     6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#create a function that will produce a subj-pred frequency data frame\n",
    "def depfreq(data):\n",
    "    #strip away alphanumerics and symbols\n",
    "    chin_only = [''.join([c for c in i if unicodedata.category(c)[:2] not in [\"L\", \"S\", \"Lu\", \"Ll\"]]) for i in data]\n",
    "    #parse\n",
    "    n = [nlp(i) for i in chin_only]\n",
    "    s_p = []\n",
    "    #extract nsubj and concatenate them with the head\n",
    "    for i in n:\n",
    "        for sents in i:\n",
    "            if sents.dep_ == \"nsubj\":\n",
    "                x = sents.text + '_' + sents.head.text\n",
    "                s_p.append(x)\n",
    "    #create df and do freq counts\n",
    "    df = pd.DataFrame(s_p, columns = ['SUBJ-PRED'])\n",
    "    freq = df[\"SUBJ-PRED\"].value_counts().reset_index()\n",
    "    freq.columns = ['SUBJ-PRED', 'FREQ']\n",
    "    return freq\n",
    "\n",
    "#run function with data and view results\n",
    "Q4_4 = depfreq(data.content)\n",
    "Q4_4.head(21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
